{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "T258O_5OR2G0"
      ],
      "machine_shape": "hm",
      "gpuType": "T4",
      "authorship_tag": "ABX9TyP866Nmij02x2jDQhVnQeMu",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/atadria/LLMs/blob/main/mistral_code_samples.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Code samples to run Mistral model"
      ],
      "metadata": {
        "id": "ZGduKHFPjyDd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# to fix: NotImplementedError: A UTF-8 locale is required. Got ANSI_X3.4-1968\n",
        "import locale\n",
        "locale.getdefaultlocale()"
      ],
      "metadata": {
        "id": "F_C7ws0ev0k7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## GGUF\n",
        "- CPU and GPU interference.\n",
        "GGUF (GPT-Generated Unified Format)"
      ],
      "metadata": {
        "id": "EQLsQw5sEPkV"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tUPWu0ivx58v"
      },
      "outputs": [],
      "source": [
        "!pip install -q ctransformers"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# fix: OSError: libcudart.so.12: cannot open shared object file: No such file or directory\n",
        "!pip uninstall -q torch\n",
        "!pip install -q torch"
      ],
      "metadata": {
        "id": "MmS9PuMZ5IRS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from ctransformers import AutoModelForCausalLM\n",
        "\n",
        "# Set gpu_layers to the number of layers to offload to GPU.\n",
        "# Set to 0 if no GPU acceleration is available on your system.\n",
        "llm = AutoModelForCausalLM.from_pretrained(\n",
        "    \"TheBloke/Mistral-7B-Instruct-v0.1-GGUF\",\n",
        "    model_file=\"mistral-7b-instruct-v0.1.Q4_K_M.gguf\",\n",
        "    model_type=\"mistral\",\n",
        "    gpu_layers=128)"
      ],
      "metadata": {
        "id": "r5EIOUWJylMp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "llm(\"</s>[INST] Jaką mamy dzisiaj pogodę w Krakowie? [/INST]\",\n",
        "    temperature=0.3,\n",
        "    max_new_tokens=50)"
      ],
      "metadata": {
        "id": "NZWLCMnU0XYM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## GPTQ\n",
        "- GPU only\n",
        "\n",
        "https://arxiv.org/abs/2210.17323"
      ],
      "metadata": {
        "id": "XrCOAhUiqonY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# restart runtime after runing this cell\n",
        "!pip install -q accelerate\n",
        "!pip install -q optimum\n",
        "!pip install -q auto-gptq"
      ],
      "metadata": {
        "id": "iS38yWC0rWsy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
        "\n",
        "model_name_or_path = \"TheBloke/Mistral-7B-Instruct-v0.1-GPTQ\"\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(model_name_or_path,\n",
        "                                             device_map=\"auto\",\n",
        "                                             trust_remote_code=False,\n",
        "                                             revision=\"main\")\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name_or_path, use_fast=True)\n",
        "\n",
        "prompt = \"Tell me about AI\"\n",
        "prompt_template=f'''<s>[INST] {prompt} [/INST]\n",
        "'''\n",
        "\n",
        "print(\"\\n\\n*** Generate:\")\n",
        "\n",
        "input_ids = tokenizer(prompt_template, return_tensors='pt').input_ids.cuda()\n",
        "output = model.generate(inputs=input_ids, temperature=0.7, do_sample=True, top_p=0.95, top_k=40, max_new_tokens=512)\n",
        "print(tokenizer.decode(output[0]))\n",
        "\n",
        "# Inference can also be done using transformers' pipeline\n",
        "\n",
        "print(\"*** Pipeline:\")\n",
        "pipe = pipeline(\n",
        "    \"text-generation\",\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    max_new_tokens=512,\n",
        "    do_sample=True,\n",
        "    temperature=0.7,\n",
        "    top_p=0.95,\n",
        "    top_k=40,\n",
        "    repetition_penalty=1.1\n",
        ")\n",
        "\n",
        "print(pipe(prompt_template)[0]['generated_text'])"
      ],
      "metadata": {
        "id": "F9opeAmerJ2a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## AutoAWQ\n",
        "- GPU only\n",
        "\n",
        "AutoAWQ is an easy-to-use package for 4-bit quantized models. AutoAWQ speeds up models by 2x while reducing memory requirements by 3x compared to FP16. AutoAWQ implements the Activation-aware Weight Quantization (AWQ) algorithm for quantizing LLMs.\n",
        "\n",
        "https://github.com/casper-hansen/AutoAWQ\n",
        "\n",
        "https://arxiv.org/abs/2306.00978"
      ],
      "metadata": {
        "id": "T258O_5OR2G0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q autoawq"
      ],
      "metadata": {
        "id": "83prNllqR1Rv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from awq import AutoAWQForCausalLM\n",
        "from transformers import AutoTokenizer, TextStreamer\n",
        "\n",
        "quant_path = \"TheBloke/Mistral-7B-Instruct-v0.1-AWQ\"\n",
        "\n",
        "# Load model\n",
        "model = AutoAWQForCausalLM.from_quantized(quant_path, fuse_layers=True, safetensors=True)\n",
        "tokenizer = AutoTokenizer.from_pretrained(quant_path, trust_remote_code=True)\n",
        "streamer = TextStreamer(tokenizer, skip_prompt=True, skip_special_tokens=True)"
      ],
      "metadata": {
        "id": "mQDaW73YLgqH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt_template = \"\"\"<s>[INST] {prompt} [/INST]\"\"\"\n",
        "prompt = \"What is the weather today in Krakow?\"\n",
        "\n",
        "tokens = tokenizer(\n",
        "    prompt_template.format(prompt=prompt),\n",
        "    return_tensors='pt'\n",
        ").input_ids.cuda()\n",
        "\n",
        "# Generate output\n",
        "generation_output = model.generate(\n",
        "    tokens,\n",
        "    streamer=streamer,\n",
        "    max_new_tokens=512\n",
        ")"
      ],
      "metadata": {
        "id": "ZuqCiPmwUSyP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## vLLM + AWQ\n",
        "- GPU only\n",
        "\n",
        "vLLM is a **FAST** and easy-to-use library for LLM inference and serving.\n",
        "\n",
        "https://github.com/vllm-project/vllm\n",
        "\n",
        "https://arxiv.org/abs/2309.06180"
      ],
      "metadata": {
        "id": "lrX5j0FVLhZh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q vllm"
      ],
      "metadata": {
        "id": "0aT22PKIioQA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from vllm import LLM, SamplingParams\n",
        "import torch\n",
        "\n",
        "llm = LLM(model=\"TheBloke/Mistral-7B-Instruct-v0.1-AWQ\",\n",
        "          quantization='awq',\n",
        "          dtype='half',\n",
        "          # max_model_len=512 fix oom error\n",
        "          max_model_len=512)\n",
        "\n",
        "sampling_params = SamplingParams(temperature=0.3,\n",
        "                                 top_p=0.95,\n",
        "                                 max_tokens=50)"
      ],
      "metadata": {
        "id": "_9fOfxkBELqn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompts = [\n",
        "    \"<s>[INST] Jaką mamy dzisiaj pogodę w Krakowie? [/INST]\",\n",
        "    \"<s>[INST] What is the weather today in Krakow? [/INST]\",\n",
        "]\n",
        "\n",
        "outputs = llm.generate(prompts, sampling_params)\n",
        "\n",
        "# Print the outputs.\n",
        "for output in outputs:\n",
        "    prompt = output.prompt\n",
        "    generated_text = output.outputs[0].text\n",
        "    print(f\"\\nPrompt: {prompt!r}, \\nGenerated text: {generated_text!r}\")"
      ],
      "metadata": {
        "id": "DvHzNfWqG95I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## bitsandbytes\n",
        "- GPU only\n",
        "- !!! more RAM is needed than free Colab tier - T4 with high RAM option is ok!!!\n"
      ],
      "metadata": {
        "id": "dYKe3W4_1Lg5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q transformers accelerate bitsandbytes"
      ],
      "metadata": {
        "id": "LwEvE48J37u3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
        "import torch\n",
        "\n",
        "device = \"cuda\" # the device to load the model onto\n",
        "\n",
        "model_name = \"mistralai/Mistral-7B-Instruct-v0.1\"\n",
        "\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.bfloat16\n",
        ")\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(model_name,\n",
        "                                             quantization_config=bnb_config)\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)"
      ],
      "metadata": {
        "id": "61MCLZSr2NO0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt_template = \"\"\"<s>[INST] {prompt} [/INST]\"\"\"\n",
        "\n",
        "prompt = \"Do you have mayonnaise recipes?\"\n",
        "\n",
        "model_inputs = tokenizer.encode(prompt_template.format(prompt=prompt),\n",
        "                                return_tensors=\"pt\").to(device)\n",
        "\n",
        "generated_ids = model.generate(model_inputs, max_new_tokens=1000, do_sample=True)\n",
        "decoded = tokenizer.batch_decode(generated_ids)\n",
        "print(decoded[0])"
      ],
      "metadata": {
        "id": "DsbEdj3G8LpC"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}